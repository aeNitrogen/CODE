---
# Slurm config bwuni gpu
name: "SLURM"   # MUST BE "SLURM"
partition: "gpu_8"  # "single" for cpu, "gpu_4" or gpu_8" for gpu
job-name: "test_uc" # this will be the experiment's name in slurm
num_parallel_jobs: 30  # max number of jobs executed in parallel
ntasks: 1   #  leave that like it is
cpus-per-task: 4   # there are 5 cores for each GPU on the gpu_8 queue and 10 per GPU on the gpu_4 queue. Never use 5! don't ask why!
mem-per-cpu: 2250 # in mb
time: 20   # in minutes
sbatch_args:   # gpus need to be explicitly requested using this
  gres: "gpu:1" #and this (specifies number of gpus requested)

---
name: DEFAULT
iterations: 3000
run_cap: 40
method: "random"
metric:
  name: "validation_loss"
  goal: "minimize"
parameters:
  architecture: "transformer"
  learning_rate:
    distribution: "q_log_uniform"
    min: 0.00001
    max: 0.2
  d_model:
    values: [256, 512, 1024, 2048, 4096, 8192]
  dropout:
    values: [0, 0.1, 0.3, 0.5]
  n_hidden_decoder:
    values: [256, 512, 2048, 8192]
  n_hidden_encoder:
    values: [256, 512, 2048, 8192]
  decoder_overlap: 5
  prediction_length: 500
  lookback_window: 150
  seed: 8055
  n_heads: 4
  individual: None